# Deep learning

This workshop focuses on training a multi-layer perceptron (MLP) to distinguish between two classes: apple and lemon. The tasks involved are:

1. Converting the images to grayscale and vectorizing them as a pre-processing step.
2. Training the MLP using stochastic gradient descent (SGD), batch gradient descent (BGD), and mini-batch gradient descent (MGD).
3. Using ReLU and sigmoid activation functions separately and measuring the performance (processing time) of each configuration.
4. Initializing all weights with zero and checking the network's behavior (loss behavior and performance).
5. Plotting the confusion matrix and loss curve for each configuration.
